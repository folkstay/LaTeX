\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}
\usepackage{amsmath, amssymb}
\usepackage{enumitem}
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

\parindent=1cm
% Векторы
\newcommand{\vect}[1]{\vec{#1}}

\begin{document}

\setcounter{page}{33}
% Создаем новый стиль списка
\newlist{hangenum}{enumerate}{1}
\setlist[hangenum]{
    label=\arabic*$^\circ$.,
    leftmargin=0pt,
    itemindent=1.25cm,
    before={\everyitem{\hangindent=\parindent}}
}

\begin{hangenum}[start=4]
    \parindent=0.7cm
    \item \textit{(Характеризация.)} Пусть $k > 1$. Система $\vect{a}_1, \ldots, \vect{a}_k$ линейно зависима $\Longleftrightarrow$ один из векторов есть линейная комбинация других.

    \item Пусть система $\vect{a}_1, \ldots, \vect{a}_k$ \ — линейно независима, система $\vect{a}_1, \ldots, \vect{a}_k,  \vect{b}$ — линейно зависима. Тогда $\vect{b}$ есть линейная комбинация $\vect{a}_1, \ldots, \vect{a}_k$.

    \item Пусть $\vect{b}$ есть линейная комбинация $\vect{a}_1, \ldots, \vect{a}_k$:
    \setcounter{equation}{1}
    \begin{equation} \label{eq:comb}
    \vect{b} = \sum_{i=1}^{k} \alpha_i \vect{a}_i.
    \end{equation}

    Представление (\ref{eq:comb}) является единственным $\Longleftrightarrow$ система $\vect{a}_1, \ldots, \vect{a}_k$ линейно независима. \textit{Эквивалентная формулировка:} представление (\ref{eq:comb}) не является единственным $\Longleftrightarrow$ система $\vect{a}_1, \ldots, \vect{a}_k$ линейно зависима.

    Два разложения вида (\ref{eq:comb}) считаются одинаковыми, если все соответствующие коэффициенты правых частей этих соотношений равны.
\end{hangenum}

\noindent \textbf{Доказательство.}

\begin{hangenum}[label=\arabic*$^\circ$.]
    
    \item $\Longleftarrow$. \quad Следует из равенства $1 \cdot \vect{0} = \vect{0}$.
    
     \leftskip=3em $\Longrightarrow$. \quad Если $\lambda \vect{a} = \vect{0}$, $\lambda \neq 0$, то $\vect{a} = \vect{0}$.

    \leftskip=0em 
    \item Для любых $\vect{a}_i$ выполнено равенство
    \[
    1 \cdot \vect{0} + 0 \cdot \vect{a}_1 + \ldots + 0 \cdot \vect{a}_k = \vect{0}.
    \]

    \item Пусть подсистема $(\vect{a}_i)$ системы $\vect{a}_1, \ldots, \vect{a}_k, \vect{b}_1, \ldots, \vect{b}_l$ линейно зависима. Это означает, что для некоторого нетривиального набора коэффициентов $\lambda_i$ выполнено равенство (1). Взяв все $\mu_j := 0$, очевидно, получим:
    \[
    \sum_{i=1}^{k} \lambda_i \vect{a}_i + \sum_{j=1}^{l} \mu_j \vect{b}_j = \vect{0}.
    \]
    Последнее равенство гарантирует линейную зависимость всей системы, так как среди коэффициентов левой части имеется хотя бы один ненулевой.

    
    \item $\Longrightarrow$. \quad Если выполнено (1), причём $\lambda_i \neq 0$ для некоторого $i$, то $\vect{a}_i$ линейно выражается через остальные векторы системы (каким образом?).

    \leftskip=3em
    $\Longleftarrow$. \quad Если для некоторого $i$
    
    \leftskip=0em
    \[
    \vect{a}_i = \sum_{j \neq i} \gamma_j \vect{a}_j,
    \]
    то, очевидно,
    \[
    1 \cdot \vect{a}_i + \sum_{j \neq i} (-\gamma_j) \vect{a}_j = \vect{0}.
    \]
    В левой части стоит нетривиальная линейная комбинация.

    \item Для некоторого набора чисел $\alpha_1, \ldots, \alpha_k, \beta$, среди которых имеется хотя бы одно ненулевое, выполнено равенство
 \newpage
%%%%%%%%%%%%%%%%%%%%%%
 \[
    \sum_{i=1}^{k} \alpha_i \vect{a}_i + \beta \vect{b} = \vect{0}.
    \]
    Простой анализ показывает, что $\beta \neq 0$ — иначе система $\vect{a}_1, \ldots, \vect{a}_k$ линейно зависима. Поэтому $\vect{b}$ линейно выражается через $\vect{a}_i$.

    \item Докажем это свойство в приведённой выше эквивалентной формулировке.

    \leftskip=2em
    $\Longrightarrow$. \quad Пусть представление (2) не является единственным: для другого набора 
        \leftskip=0em
        коэффициентов выполнено ещё равенство 
    

    \[
    \vect{b} = \sum_{i=1}^{k} \beta_i \vect{a}_i.
    \]
    Вычитая из (2) последнее представление, получим
    \[
    \sum_{i=1}^{k} (\alpha_i - \beta_i) \vect{a}_i = \vect{0}.
    \]
    Среди коэффициентов левой части есть хотя бы один ненулевой. Поэтому система $\vect{a}_1, \ldots, \vect{a}_k$ линейно зависима.
    
    \parindent=0.7cm
    $\Longleftarrow$. \quad Пусть система $\vect{a}_1, \ldots, \vect{a}_k$ линейно зависима, тогда с нетривиальным набором коэффициентов $\lambda_1, \ldots, \lambda_k$ выполнено равенство (1). Прибавим его к равенству (2) и запишем результат в виде
    \[
    \vect{b} = \sum_{i=1}^{k} (\alpha_i + \lambda_i) \vect{a}_i.
    \]
    Нетрудно понять, что мы получили новое представление для $\vect{b}$ — хотя бы при одном значении $i$ выполнено $\alpha_i + \lambda_i \neq \alpha_i$ \ , поэтому последнее соотношение отличается от (2).
    
    Свойства доказаны.
\end{hangenum}

\vspace{4ex}
{\centering
\large
\parbox{0.85\textwidth}{\centering
\textbf{3.3. Связь линейной зависимости в $V_n$ с коллинеарностью и компланарностью}
}\par}
\vspace{4ex} % Увеличьте значение по необходимости


Дадим в этом пункте простую геометрическую интерпретацию линейной зависимости и независимости в пространствах геометрических векторов. Как вы помните, основное определение является алгебраическим.

\textbf{Теорема.} \textit{(0) Один вектор образует линейно зависимую систему $\Longleftrightarrow$ этот вектор является нулевым.}

\begin{enumerate}[label=\textit{(\arabic*)},leftmargin=0pt,itemsep=0pt,start=1,itemindent=1.25cm]
    \item \textit{Два вектора линейно зависимы \ $\Longleftrightarrow$ \ они коллинеарны.}
    \item \textit{Три вектора линейно зависимы \ $\Longleftrightarrow$ \ они компланарны.}
    \item \textit{Любые два вектора из $V_1$, три вектора из $V_2$, четыре вектора из $V_3$ 
    (и большее число векторов во всех ситуациях) линейно зависимы.}
\end{enumerate}

 \newpage
%%%%%%%%%%%%%%%%%%%%%%
\parindent=0.7cm
\textbf{Доказательство} вполне элементаpно; логическая схема использует свойства
линейной зависимости пpедыдущего пункта. Часть (0) совпадает со свойством 1$^\circ$ и
пpиводится для полноты.

(1) Два вектоpа линейно зависимы $\Longleftrightarrow$ они связаны скаляpным множителем
(хаpактеpизация из свойства 4$^\circ$
) $\Longleftrightarrow$ они коллинеаpны.

(2) $\Longrightarrow$. \quad Если тpи вектоpа линейно зависимы, то один из них есть линейная
комбинация двух дpугих (свойство 4$^\circ$
) и, значит, лежит в плоскости этих двух
вектоpов (геометpия опеpаций).

$\Longleftarrow$. \quad Пусть $\vect{a}, \vect{b}, \vect{c}$ компланаpны.
Если $\vect{a}, \vect{b}$  коллинеаpны, то они линейно за-висимы по
пpедыдущему; \ тогда и вся система линейно зависима  (свойство 3$^\circ$).
Если
же $\vect{a}, \vect{b}$ неколлинеаpны, то $\vect{c}$  нетpудно пpедставить в виде $c = \alpha\vec{a} + \beta\vec{b}$  (выполните
соответствующее геометpическое постpоение.)

(3) Замечание в скобках связано со свойством 3$^\circ$.  Далее, два вектоpа из $V_1$ коллинеаpны, тpи вектоpа из $V_2$ компланаpны и, значит, линейно зависимы по пpедыдущему.

Пусть $\vec{a},\vec{b},\vec{c},\vec{d} \in V_3$. Если $\vec{a},\vec{b},\vec{c}$ компланарны, то они линейно зависимы (часть (2));  тогда и вся система из четырёх векторов линейно зависима (свойство 3$^\circ$).

\noindent
Если же $\vec{a}, \vec{b}, \vec{c}$ некомпланарны, то $\vec{d}$ нетрудно представить в виде $\vec{d} = \alpha\vec{a} + \beta\vec{b} + \gamma\vec{c}$ (выполните соответствующее геометрическое построение).

Теоpема доказана.

Изложенная геометpическая интеpпpетация линейной зависимости в $V_n$ даёт
возможность не выходить здесь за pамки понятий коллинеаpности и компланаpности. Пpеимущество исходного алгебpаического опpеделения линейной зависимости
состоит в \textit{степени общности} — этот подход без изменений пеpеносится в дальнейшем на пpоизвольные линейные пpостpанства.

\vspace{4ex}
{\centering
\large
\parbox{1\textwidth}{\centering
\textbf{3.4. Решение задачи о линейной зависимости векторов из $\mathbb{R}^n$, $n \in \mathbb{N}$. Линейная зависимость $k$ произвольных $n$-мерных векторов \linebreak при $k > n$}
}\par}
\vspace{4ex}

Пусть $a^{(1)}, \ldots, a^{(k)}, {b}$ — некоторые $n$-мерные векторы; их компоненты в этом пункте из технических соображений удобнее записать по столбцам:

\[
{a}^{(1)} = \begin{pmatrix}
a_{11} \\
a_{21} \\
\vdots \\
a_{n1}
\end{pmatrix}, \quad 
\ldots, \quad 
{a}^{(k)} = \begin{pmatrix}
a_{1k} \\
a_{2k} \\
\vdots \\
a_{nk}
\end{pmatrix}, \quad 
{b} = \begin{pmatrix}
b_1 \\
b_2 \\
\vdots \\
b_n
\end{pmatrix}.
\]

\parindent=0.6cm
Вопрос о представлении вектора ${b}$ в виде линейной комбинации векторов ${a}^{(1)}, \ldots, \\ {a}^{(k)}$ (или, как говорят, о \textit{разложении} одного вектора по системе других) есть вопрос о справедливости и коэффициентах равенства
\[
\lambda_1 {a}^{(1)} + \ldots + \lambda_k {a}^{(k)} = {b},
\]
%%%%%%%%%%%%%%%%%%%
\newpage
или в компонентах
\[
\lambda_1 \ \begin{pmatrix}
a_{11} \\
a_{21} \\
\vdots \\
a_{n1}
\end{pmatrix} + \ldots + \lambda_k \ \begin{pmatrix}
a_{1k} \\
a_{2k} \\
\vdots \\
a_{nk}
\end{pmatrix} = \begin{pmatrix}
b_1 \\
b_2 \\
\vdots \\
b_n
\end{pmatrix}
\].

Таким образом, вопрос сводится к анализу системы $n$ линейных уравнений с $k$
неизвестными $\lambda_1, \ldots, \lambda_k$. Расширенная матрица этой системы имеет вид
\[
\left(
\begin{array}{cccc|c}
a_{11} & a_{12} & \ldots & a_{1k} & b_1 \\
a_{21} & a_{22} & \ldots & a_{2k} & b_2 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
a_{n1} & a_{n2} & \ldots & a_{nk} & b_n \\
\end{array}
\right)
\]

Заметим, что система уравнений может быть и несовместной — в этой ситуации
${b}$ нельзя представить в виде линейной комбинации ${a}^{(i)}$.

Обратимся теперь к анализу линейной зависимости системы из $k$ данных $n$-мерных векторов ${a}^{(1)}, \ldots, {a}^{(k)}$. Ясно, что в предыдущих рассмотрениях надо взять
${b} = {0}$. В этом случае система уравнений с неизвестными $\lambda_i$ будет однородной и, следовательно, всегда совместной. Если эта однородная система уравнений является
определённой, то есть обладает только нулевым решением

\[
\lambda_1 = \ldots = \lambda_k = 0,
\]
то исходная система векторов ${a}_1, \ldots, {a}_k$ является линейно независимой. Если же
система однородных уравнений с матрицей \ $(a_{ij})$, $i = 1, \ldots, n$, \ $j = 1, \ldots, k$, является неопределённой, то исходная система $n$-мерных векторов является линейно
зависимой. Эти простые рассуждения приводят к следующему важному результату.



\textbf{Теорема.} \textit{Пусть $k > n$. Система, состоящая из $k$ произвольных $n$-мерных
векторов, является линейно зависимой.}


\textbf{Доказательство} состоит из одной фразы: однородная система $n$ линейных
уравнений с $k$ неизвестными является неопределённой.

Очевидно, что в каждом из пространств $\mathbb{R}^n$ существуют линейно независимые
системы, состоящие ровно из $n$ векторов; подумайте над простейшим примером.

\vspace{1ex}
\textsl{Упражнение 1. Сформулировать аналог теоремы для пространства матриц $M_{m,n}$.
Какая система из $mn$ матриц является линейно независимой?}

\textsl{Упражнение 2. Доказать, что для любой квадратной матрицы $\mathbf{A}$ порядка $n$
существует ненулевой аннулирующий многочлен. (\textit{Указание}. Воспользоваться линейной зависимостью в $M_n$ матриц $\mathbf{E}$, $\mathbf{A}$, $\mathbf{A}^2$, $\ldots$, $\mathbf{A}^{n^2}$.)}
\vspace{2ex}

\vspace{4ex}
{\centering
\large
\parbox{1\textwidth}{\centering
\textbf{3.5. Базис и координаты в $V_n$. Характеризация базисов в $V_1$, $V_2$, $V_3$. Размерность. Изоморфизм $V_n$ и $\mathbb{R}^n$, $n = 1, 2, 3$}
}\par}
\vspace{4ex}

%%%%%%%%%%%%%%%%%%%
\newpage
Наличие базиса — важнейшее характеристическое свойство нетривиальных \textit{конечномерных линейных пространств}, простейшими примерами которых являются
рассматриваемые в этой части пространства геометрических векторов, $n$-мерных
векторов и матриц.

Естественно, в определении базиса в пространствах $V_n$ можно обойтись простыми геометрическими понятиями и действовать в стиле характеризации приводимой
ниже теоремы. Однако, опять исходя из соображений \textit{общности}, мы предпочитаем
алгебраический подход.

Поэтому определение базиса и координат без изменений переносится на пространства $\mathbb{R}^n$ и $M_{m,n}$.

\textbf{Определение.} Базисом $V_n$ называется конечная совокупность векторов, которая обладает свойствами:
\begin{enumerate}
    \item эта система линейно независима;
    \item каждый вектор $V_n$ есть линейная комбинация векторов этой системы.
\end{enumerate}
Коэффициенты разложения вектора $\vec{x}$ по базисным векторам называются координатами $\vec{x}$ в этом базисе.

\textbf{Теорема.} \textit{(Характеризация.) (1) Всякий базис $V_1$ состоит из одного ненулевого вектора. При этом любой ненулевой вектор из $V_1$ образует базис.}

\textit{(2) Всякий базис $V_2$ состоит из двух неколлинеарных векторов. При этом любая пара неколлинеарных векторов из $V_2$ образует базис.}

\textit{(3) Всякий базис $V_3$ состоит из трёх некомпланарных векторов. При этом любая тройка некомпланарных векторов $V_3$ образует базис.}

\textbf{Доказательство.} Остановимся для примера на части (2); части (1), (3) рассмотрите самостоятельно.

Пусть сначала имеется некоторый базис $V_2$. Так как система из трёх (и большего числа) векторов $V_2$ линейно зависима, то базис может содержать один ненулевой или пару неколлинеарных векторов. Первый вариант отпадает — векторы, неколлинеарные данному, через него линейно не выражаются.

Пусть теперь $\vec{a}, \vec{b}$ — любая фиксированная пара неколлинеарных векторов $V_2$. Ясно, что эти векторы линейно независимы. Если $\vec{x} \in V_2$ — произвольный вектор, то простое геометрическое построение означает, что
\[
\vec{x} = \alpha \vec{a} + \beta \vec{b}.
\]
Поэтому $\vec{a}, \vec{b}$ — базис $V_2$.

\textbf{Следствие.} \textit{Каждый базис $V_n$ состоит из $n$ векторов.}

Число $n$ — количество элементов базиса $V_n$ — называется \textit{размерностью} $V_n$ и обозначается $\dim V_n$. Таким образом, результат следствия означает, что $\dim V_n = n$.

\textit{Упражнение 1.} Приведите примеры базисов в пространствах $\mathbb{R}^n$ и $M_{m,n}$.

Пусть $\vec{x} \in V_n$, ~$\vec{e}_1, \ldots,  \vec{e}_n$ \ — некоторый фиксированный базис $V_n$. Здесь $n$ — любое из чисел 1, 2, 3. Будем использовать запись
\[
\vec{x} = \alpha_1 \vec{e}_1 + \ldots + \alpha_n \vec{e}_n = \{\alpha_1, \ldots, \alpha_n\}.
\]

Числа $\alpha_1, \ldots, \alpha_n$ — координаты $\vec{x}$ в данном базисе — определены единственным образом, см. свойство 6$^\circ$ линейной зависимости из пункта 3.2. Кроме того, если

%%%%%%%%%%%%%%%%%%%%
$\lambda \in \mathbb{R}$, $\vec{y} = \{\beta_1, \ldots, \beta_n\}$, то, как нетрудно понять,
\[
\vec{x} + \vec{y} = \{\alpha_1 + \beta_1, \ldots, \alpha_n + \beta_n\},
\]
\[
\lambda\vec{x} = \{\lambda\alpha_1, \ldots, \lambda\alpha_n\}.
\]

\noindent Эти равенства соответствуют \textit{действиям с векторами в координатах.}

Рассмотрим теперь соответствие между геометрическими векторами $\vec{x}$ из $V_n$ и
$n$-мерными векторами $x$ из $\mathbb{R}^n$, осуществляемое по простому правилу:
\[
\vec{x} \longleftrightarrow x = (\alpha_1, \ldots, \alpha_n), \quad \text{если} \quad \vec{x} = \{\alpha_1, \ldots, \alpha_n\}.
\]

Иначе говоря, вектору $\vec{x}$ сопоставляется набор его координат, обозначаемый
через $x$. Сказанное выше означает, что это соответствие взаимно-однозначно и
сохраняет операции сложения и умножения на число. Последнее есть вольная интерпретация следующей записи:
\[
\text{если } \quad \vec{x} \longleftrightarrow x,\ \quad \vec{y} \longleftrightarrow y,\ \quad \lambda \in \mathbb{R},
\]
\[
\text{то } \quad \vec{x} + \vec{y} \longleftrightarrow x + y,\ \quad \lambda\vec{x} \longleftrightarrow \lambda x.
\]

Соответствие $\longleftrightarrow$ является примером \textit{изоморфизма}. Говорят также, что пространства $V_n$ и $\mathbb{R}^n$ \textit{изоморфны} и записывают это, например, так:
\[
V_n \simeq \mathbb{R}^n, \quad n = 1, 2, 3.
\]

\textit{Итак, изоморфизм — это взаимно-однозначное соответствие между элементами двух линейных пространств, сохраняющее основные операции.}

Переход от одного объекта к другому, изоморфному первому, позволяет решать
задачи,поставленные для первого объекта, с помощью техники, разработанной для
второго объекта. (В широком смысле, два объекта изоморфны, если они имеют одинаковую структуру или форму — почти буквальный перевод с греческого термина
\textsl{изоморфизм.})

В нашей ситуации это позволяет решать геометрические задачи в числах. Как
пример, отметим анализ линейной зависимости геометрических векторов в координатах.

\textsl{Упражнение 2. Показать, что при изоморфизме линейно независимой системе в
$V_n$ соответствует линейно независимая система в $\mathbb{R}^n$, и наоборот.}

\textsl{В настоящем тексте мы не вводим общего понятия изоморфизма линейных пространств; однако некоторые простые ситуации могут быть рассмотрены по аналогии.}

\textsl{Упражнение 3. При каком натуральном $k$ имеет место изоморфизм $M_{m,n} \simeq \mathbb{R}^k$?
Как осуществить в этой ситуации взаимно-однозначное соответствие, сохраняющее
операции сложения и умножения на число? Какова роль базисов пространств при
установлении изоморфизма?}

\newpage
%%%%%%%%%%%%%%%%%
\vspace{4ex}
{\centering
\LARGE
\parbox{1\textwidth}{\centering
\textbf{4. Определители}
}\par}
\vspace{4ex}

У истоков истории понятия определителя стоят великий немецкий математик
Готфрид Вильгельм Лейбниц (G.W. Leibniz) и японский математик Сёки Кова —
ими независимо предложена идея определителя (в 1678 г. и 1683 г. соответственно).

Первые публикации принадлежат Крамеру (G. Cramer, 1750) и Лагранжу
(J.L. Lagrange, 1770). Термин \textsl{детерминант} (определитель) введён Гауссом
(C. Gauss, 1801); современное обозначение принадлежит Кэли (A. Cayley, 1841).

\textit{Теория определителей} создана в конце 18 – первой половине 19 столетий трудами Вандермонда (A.T. Vandermonde), Лапласа (P.S. Laplace), Коши (A.L. Cauchy)
и Якоби (C.G. Jacobi).

Самой важной из работ в этой области является статья Карла Густава Якоби
\textsl{``De formatione et proprietatibus determinantium''}

(``О построении и свойствах определителей'' 1841), которая сделала теорию определителей общим достоянием математиков. Чтобы воздать должное достижениям
Якоби в алгебре, Сильвестр назвал якобианом важный функциональный определитель.

Из многочисленных приложений определителей отметим \textit{решение систем линейных уравнений и вычисление объёмов}. В различных разделах математики рассматриваются, разумеется, и другие связанные с ними задачи.

В литературе отражены следующие подходы к построению определителей.

1$^\circ$. Индукция по порядку $n$ определителя (разложение по строке или столбцу).

2$^\circ$. Введение определителя как функции строк матрицы с некоторыми заданными свойствами. В пункте 4.2 эта функция обозначается как $F(X_1, \ldots, X_n)$.

3$^\circ$. Определитель как обобщённый ориентированный объём.

4$^\circ$. Прямое определение через перестановки. При таком подходе содержание
пунктов 1$^\circ$ – 3$^\circ$ имеет вид следствий или свойств.

В настоящем тексте используется последний подход. Он реализован, например,
в учебниках А.И. Кострикина [13] и А.Г. Куроша [16].

\vspace{4ex}
{\centering
\large
\parbox{1\textwidth}{\centering
\textbf{4.1. Пеpестановки и инвеpсии. Свойства пеpестановок.
Опpеделитель поpядка }\textit{n}
}\par}
\vspace{4ex} 

\textit{Перестановкой} чисел $1, 2, \ldots, n$ называется их некоторое расположение в строку. \ Иначе говоря, перестановка порядка \ $n$ есть упорядоченный набор $\alpha ~ :=  \ (\alpha_1, \ldots, \alpha_n)$ такой, что все компоненты $\alpha_j$ попарно различны и принадлежат множеству $W_n := \{1, \ldots, n\}$.

С перестановкой $\alpha$ естественным образом связывают понятие \textit{подстановки порядка $n$}, то есть взаимно-однозначного отображения множества $W_n$ на себя. Подстановка, обозначаемая тем же символом $\alpha$, изображается в виде двустрочной таблицы
\[
\alpha = \begin{pmatrix}
1 & 2 & \ldots & n \\
\alpha_1 & \alpha_2 & \ldots & \alpha_n
\end{pmatrix}.
\]

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Имеется в виду, что $j \mapsto \alpha_j$ или $\alpha_j := \alpha(j)$ (как и всегда для функций натурального аргумента).

На множестве всех подстановок порядка $n$ можно рассмотреть \textit{операцию суперпозиции} $\circ$, определяемую, как обычно, равенством
\[
\alpha \circ \beta(j) := \alpha(\beta(j)), \quad j = 1, \ldots, n.
\]

\textit{Пример 1.} Если $n = 4$ и
\[
\alpha = \begin{pmatrix} 1 & 2 & 3 & 4 \\ 2 & 3 & 4 & 1 \end{pmatrix}, \quad 
\beta = \begin{pmatrix} 1 & 2 & 3 & 4 \\ 3 & 2 & 4 & 1 \end{pmatrix},
\]
то
\[
\alpha \circ \beta = \begin{pmatrix} 1 & 2 & 3 & 4 \\ 4 & 3 & 1 & 2 \end{pmatrix} \neq 
\beta \circ \alpha = \begin{pmatrix} 1 & 2 & 3 & 4 \\ 2 & 4 & 1 & 3 \end{pmatrix}.
\]

\textsl{Упражнение 1.Проверить, что операция суперпозиции ассоциативна и для любой подстановки $\alpha$ существует \textit{обратная подстановка} $\alpha^*$, то есть такая, что $\alpha \circ \alpha^* = \alpha^* \circ \alpha = e$. Здесь $e$ — \textit{тождественная подстановка}; для неё $e(j) = j$, в связи с чем $\alpha \circ e = e \circ \alpha = \alpha$ для любой $\alpha$. Выполнение этих свойств означает, что относительно операции $\circ$ совокупность $S_n$ всех подстановок порядка $n$ образует группу (некоммутативную при $n > 2$), называемую \textit{группой подстановок.} См. также пункт 12.1.}

В дальнейшем $\alpha = (\alpha_1, \ldots, \alpha_n)$ считается перестановкой. Нам потребуется несколько простых определений.

Пусть $i < k$. Будем говорить, что компоненты перестановки $\alpha_i$ и $\alpha_k$ образуют \textit{инверсию}, если $\alpha_i > \alpha_k$, и образуют \textit{порядок}, если $\alpha_i < \alpha_k$. Число всех инверсий в перестановке $\alpha$ обозначим $s(\alpha)$. В зависимости от чётности $s(\alpha)$ перестановка $\alpha$ называется \textit{чётной} или \textit{нечётной}.

\textit{Транспозицией} называется такая операция с перестановкой, при которой меняются местами некоторые две её компоненты, а остальные остаются на своих местах.

Наиболее сложным выглядит понятие обратной перестановки $\alpha^*$. Пусть $\alpha$ — некоторая перестановка. Построим новую перестановку $\beta$ по правилу: для всех значений $j = 1, \ldots, n$ $\beta_j$ есть номер числа $j$ как компоненты перестановки $\alpha$. Иначе говоря, $\alpha_{\beta_j} = j$ при всех $j$. Так как при таком подходе $\beta_{\alpha_j}$ есть номер числа $\alpha_j$ в перестановке $\alpha$, то есть $j$, то одновременно $\beta_{\alpha_j} = j$. Таким образом, выполнены равенства
\[
\alpha_{\beta_j} = \beta_{\alpha_j} = j, \quad j = 1, \ldots, n. \tag{1}
\]

\textit{Пример 2.} Пусть $\alpha = (2, 3, 4, 1)$. Тогда $\beta = (4, 1, 2, 3)$.

Построенная таким образом перестановка $\beta$ называется \textit{обратной к перестановке} $\alpha$ и обозначается $\alpha^*$.

\textit{Замечание.} Ситуация заметно проясняется, если в этом месте привлечь к рассмотрению подстановки. На языке отображений равенства (1) означают, что $\alpha \circ \beta = \beta \circ \alpha = e$, то есть $\alpha^*$ есть обратная к $\alpha$ подстановка, см. упражнение 1.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\vspace{2ex}
{\centering
\large
\itshape
\spaceskip 2pt
\textsl{С в о й с т в а ~ п е р е с т а н о в о к}
\par}
\vspace{2ex}

1$^\circ$. Количество всех перестановок порядка $n$ равно $n!$.

\vspace{0.5ex}
2$^\circ$. Любая транспозиция меняет чётность перестановки.

\vspace{0.5ex}
3$^\circ$. Число чётных перестановок порядка $n > 1$ совпадает с числом нечётных и равно $n!/2$.

\vspace{0.5ex}
4$^\circ$. Если $\beta = \alpha^*$, то $\beta^* = \alpha$. Иначе говоря, $(\alpha^*)^* = \alpha$.

\vspace{0.5ex}
5$^\circ$. $s(\alpha^*) = s(\alpha)$.

\textbf{Доказательство.} 1$^\circ$ легко устанавливается индукцией по $n$.

Утверждение свойства 2$^\circ$ очевидно для транспозиции соседних компонент. Общий результат следует из того, что произвольная транспозиция может быть получена в результате выполнения нечётного числа транспозиций соседних членов. Если между теми компонентами, которые требуется поменять местами, имеется $k$ членов, нетрудно найти нужную цепочку из $2k + 1$ транспозиций соседних компонент.

Приведём элегантное доказательство свойства 3$^\circ$. На множестве всех чётных перестановок порядка $n$ рассмотрим операцию транспозиции первых двух компонент. При выполнении этой операции каждая чётная перестановка переходит в нечётную; различные перестановки переходят в различные. Поэтому общее число $M$ чётных перестановок и общее число $N$ нечётных перестановок связаны неравенством $M \leq N$. Аналогичное рассуждение даёт $N \leq M$. Поэтому $M = N = n!/2$.

Для доказательства 4$^\circ$ положим $\beta = \alpha^*$ и $\gamma = \beta^*$. Тогда в соответствии с равенствами (1) будет выполнено
\[
\alpha_{\beta_j} = j = \gamma_{\beta_j}, \quad j = 1, \ldots, n,
\]
в связи с чем $\alpha = \gamma$.

Наконец, установим интересное свойство 5$^\circ$. Пусть компоненты $\alpha_i$, $\alpha_k$ образуют инверсию в перестановке $\alpha$. Это означает, что одновременно $i < k$ и $\alpha_i > \alpha_k$. Заметим, что тогда числа $i$ и $k$ составляют инверсию в обратной перестановке $\alpha^*$, поскольку в $\alpha^*$ они стоят на местах $\alpha_i$ и $\alpha_k$ соответственно. Таким образом, каждой инверсии в перестановке $\alpha$ соответствует некоторая инверсия в перестановке $\alpha^*$. Поэтому $s(\alpha) \leq s(\alpha^*)$. В силу двойственности свойства 4$^\circ$ выполняется и противоположное неравенство. Поэтому $s(\alpha^*) = s(\alpha)$.

Свойства доказаны.

Перейдём теперь к центральному определению этого пункта. Пусть $A = (a_{ij})$ — квадратная матрица порядка $n$, то есть $A \in M_n$.

\textbf{Определение.} \textit{Число}
\[
|A| = \det(A) := \sum_{\alpha} (-1)^{s(\alpha)} a_{1\alpha_1} a_{2\alpha_2} \ldots a_{n\alpha_n} \tag{2}
\]
\textit{называется определителем матрицы \textbf{A}. Суммирование в (2) осуществляется по множеству всех перестановок порядка $n$.}

Анализ формулы (2) показывает, что \textbf{|A|} есть алгебраическая сумма произведений $n$ элементов матрицы, в каждом из которых присутствует ровно один элемент

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\noindent из каждой строки и ровно один элемент из каждого столбца. Всего таких произ-ведений $a_{1\alpha_1} \ldots a_{n\alpha_n}$ — так называемых \textbf{членов определителя} — набирается ровно $n!$; каждое произведение соответствует некоторой перестановке $\alpha$ порядка $n$. В зависимости от чётности соответствующей перестановки член определителя снабжается знаком плюс или минус. Например, произведение элементов главной диагонали матрицы входит в определитель со знаком плюс, так как соответствующая перестановка $\alpha = (1, 2, \ldots, n)$ является чётной (для неё $s(\alpha) = 0$).

\textsl{Упражнение 2. Убедиться, что при $n = 1, 2, 3$ формула (2) содержит привычные выражения для определителей соответствующего порядка. Составить графическое правило для вычисления определителя четвёртого порядка.}

\textsl{Упражнение 3. Дополнить произведение элементов $a_{13}a_{24}a_{35}a_{46}a_{57}$ определителя седьмого порядка так, чтобы получить член этого определителя, входящий в него со знаком минус.}

\vspace{4ex}
{\centering
\large
\parbox{1\textwidth}{\centering
\textbf{4.2. Свойства опpеделителя. Вычисление методом Гаусса
 }
}\par}
\vspace{4ex} 

Ряд из приводимых в этом пункте свойств характеризует определитель как
функцию строк матрицы.

Пусть $X_1, \ldots, X_n \in \mathbb{R}^n$ — строки матрицы $\mathbf{A} = (a_{ij}) \in M_n$. Определим функцию
$F$ равенством
\[
F(X_1, \ldots, X_n) := |\mathbf{A}|.
\]
Таким образом,
\[
F: \underbrace{\mathbb{R}^n \times \ldots \times \mathbb{R}^n}_{n} \to \mathbb{R}.
\]

Содержание этого пункта, в частности, означает, что $F$ — \textit{линейная по каждому
аргументу и кососимметричная функция} (свойства 1–2 и 6 соответственно).

\textbf{Свойство 1.} \textit{Пусть $a_{kj} = b_{kj} + c_{kj}$, $j = 1, \ldots, n$. Тогда}
\[
|A| = (k)\begin{vmatrix}
\ldots & \ldots & \ldots \\
b_{k1} & \ldots & b_{kn} \\
\ldots & \ldots & \ldots
\end{vmatrix} + (k)\begin{vmatrix}
\ldots & \ldots & \ldots \\
c_{k1} & \ldots & c_{kn} \\
\ldots & \ldots & \ldots
\end{vmatrix} \tag{3}
\]
\textit{(выделены $k$-е строки определителей; остальные — такие же, как в $A$).}

\textit{Аналогичный результат справедлив, когда одна из строк определителя представляется в виде суммы $m$ строк.}

Свойство 1 означает, что функция $F$ \textit{аддитивна} по каждому аргументу. Напри-мер, для $k = 1$ и произвольного $m \in \mathbb{N}$ имеем:
\[
F\left(\sum_{i=1}^m Y_i, X_2, \ldots, X_n\right) = \sum_{i=1}^m F(Y_i, X_2, \ldots, X_n).
\]

\textbf{Доказательство.} Равенство (3) имеет вид
\[
|A| = \sum_{\alpha} (-1)^{s(\alpha)} a_{1\alpha_1} \ldots (b_{k\alpha_k} + c_{k\alpha_k}) \ldots a_{n\alpha_n} =
\]

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\[
= \sum_{\alpha} (-1)^{s(\alpha)} a_{1\alpha_1} \ldots b_{k\alpha_k} \ldots a_{n\alpha_n} + \sum_{\alpha} (-1)^{s(\alpha)} a_{1\alpha_1} \ldots c_{k\alpha_k} \ldots a_{n\alpha_n}.
\]
Общий случай получается индукцией по $m$.

\textbf{Свойство 2.} \textit{При умножении некоторой одной строки на число $c$ определитель умножается на это число.}

Это означает, что $F$ — функция, \textit{однородная} по каждому аргументу. Например,
$F(cX_1, X_2, \ldots, X_n) = cF(X_1, X_2, \ldots, X_n)$.

Доказательство содержится в равенстве
\[
\sum_{\alpha} (-1)^{s(\alpha)} a_{1\alpha_1} \ldots (ca_{k\alpha_k}) \ldots a_{n\alpha_n} = c \sum_{\alpha} (-1)^{s(\alpha)} a_{1\alpha_1} \ldots a_{n\alpha_n}.
\]

\textbf{Свойство 3.}\textit{ Определитель матрицы с нулевой строкой равен нулю}. Например, $F(0, X_2, \ldots, X_n) = 0$; в левой части $0 = (0, \ldots, 0)$.

Следует из определения или предыдущего свойства (при $c = 0$).

\textbf{Свойство 4.} \textit{Определитель, содержащий две одинаковые строки, равен нулю.} Например, $F(X, X, X_3, \ldots, X_n) = 0$.

\textbf{Доказательство.} Пусть $i < k$ и $a_{ij} = a_{kj}$ при всех $j = 1, \ldots, n$.

Рассмотрим произвольную перестановку $\alpha$ и ту перестановку $\beta$, которая получается из $\alpha$ транспозицией $i$-й и $k$-й компонент. Сумма двух соответствующих членов определителя, взятых с нужными знаками, равна нулю. Действительно, $s(\beta)$ отличается от $s(\alpha)$ на 1. Поэтому
\begin{align*}
&(-1)^{s(\beta)} a_{1\beta_1} \ldots a_{i\beta_i} \ldots a_{k\beta_k} \ldots a_{n\beta_n} = \\
&= -(-1)^{s(\alpha)} a_{1\alpha_1} \ldots a_{i\alpha_k} \ldots a_{k\alpha_i} \ldots a_{n\alpha_n} = \\
&= -(-1)^{s(\alpha)} a_{1\alpha_1} \ldots a_{k\alpha_k} \ldots a_{i\alpha_i} \ldots a_{n\alpha_n} = \\
&= -(-1)^{s(\alpha)} a_{1\alpha_1} \ldots a_{i\alpha_i} \ldots a_{k\alpha_k} \ldots a_{n\alpha_n}.
\end{align*}
В связи с этим $|\mathbf{A}|$ есть сумма $N = n!/2$ слагаемых, равных 0.

\textbf{Следствие 1.} \textit{Определитель, содержащий две пропорциональные строки, равен нулю.}

\textbf{Следствие 2.} \textit{Определитель, у которого одна из строк есть линейная комбинация других, равен нулю.
}

Для доказательства достаточно использовать свойства 1, 2 и 4. Следствие 2 (обобщающее следствие 1) означает, что, в частности,
\[
F\left(\sum_{j=2}^n c_jX_j, X_2, \ldots, X_n\right) = 0, \quad c_j \in \mathbb{R}.
\]

\textbf{Свойство 5.}\textit{ Определитель не изменится, если к любой его строке прибавить произвольную линейную комбинацию других.}

Сразу следует из свойства 1 и следствия 2. Таким образом, например,
\[
F\left(X_1 + \sum_{j=2}^n c_jX_j, X_2, \ldots, X_n\right) = F(X_1, X_2, \ldots, X_n), \quad c_j \in \mathbb{R}.
\]

\textbf{Свойство 6.} \textit{При перестановке двух строк определитель меняет знак.}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Таким образом, $F(X_1, \ldots, X_n)$ меняет знак при любой перестановке пары аргументов, например,
\[
F(X, Y, X_3, \ldots, X_n) = -F(Y, X, X_3, \ldots, X_n). \tag{4}
\]
Такие функции $F$ называют \textit{кососимметричными}.

\textbf{Доказательство.} Из свойств 1 и 4 получаем
\[
\begin{aligned}
0 &= F(X + Y, X + Y, \ldots) = F(X, X + Y, \ldots) + F(Y, X + Y, \ldots) = \\
&= F(X, X, \ldots) + F(X, Y, \ldots) + F(Y, X, \ldots) + F(Y, Y, \ldots) = \\
&~~~~~~~~~~~~~~~~~~~= F(X, Y, \ldots) + F(Y, X, \ldots),
\end{aligned}
\]
что даёт равенство (4). Общий случай получается аналогично.

\textbf{Свойство 7.} \textit{Определитель не меняется при транспонировании:}
\[
|\mathbf{A}^T| = |\mathbf{A}|.
\]

Поэтому все свойства, сформулированные выше для строк, справедливы и для столбцов.

\textbf{Доказательство.} Обозначим $\mathbf{A}^T = (a'_{ij})$. Тогда
\[
\begin{aligned}
~~~~~~~~~~~~|\mathbf{A}^T| &= \sum_{\alpha} (-1)^{s(\alpha)} a'_{1\alpha_1} \ldots a'_{n\alpha_n} = 
\end{aligned}
\]

\[
\begin{aligned}
&= \sum_{\alpha} (-1)^{s(\alpha)} a_{\alpha_1 1} \ldots a_{\alpha_n n} = \sum_{\alpha} (-1)^{s(\alpha)} a_{\alpha_1 \beta_{\alpha_1}} \ldots a_{\alpha_n \beta_{\alpha_n}}.
\end{aligned}
\]
В последнем выражении $\beta := \alpha^*$ — перестановка, обратная к $\alpha$. Мы использовали то, что для всех $j = 1, \ldots, n$ выполнено $j = \beta_{\alpha_j}$, см. равенство (1) предыдущего пункта.

Учтём теперь, что по свойствам перестановок $s(\alpha) = s(\beta)$. Упорядочение сомножителей по первому индексу и замена $\sum_{\alpha}$ на $\sum_{\beta}$ даёт с учётом предыдущего
\[
|\mathbf{A}^T| = \sum_{\beta} (-1)^{s(\beta)} a_{1\beta_1} \ldots a_{n\beta_n} = |\mathbf{A}|.
\]

\textbf{Свойство 8.}\textit{ Определитель треугольной матрицы равен произведению элементов главной диагонали.}

Под треугольной понимается матрица $\mathbf{A}$, удовлетворяющая условию $a_{ij} = 0$ при $i > j$ (\textit{верхняя треугольная матрица}) или тому же условию при $i < j$ (\textit{нижняя треугольная матрица}).

\textbf{Доказательство.} Ясно, что произведение $a_{11} \ldots a_{nn}$ входит в $|\mathbf{A}|$ со знаком плюс. Остаётся заметить, что все остальные члены определителя равны 0.

На использовании указанных свойств базируется \textit{метод Гаусса вычисления определителей}. В основе этого метода лежит возможность приведения произвольной квадратной матрицы к (верхнему) треугольному виду с помощью цепочки элементарных преобразований трёх следующих типов:

(1) перестановка строк;

\end{document}